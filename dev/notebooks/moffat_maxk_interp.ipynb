{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac27263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from functools import partial  # noqa: E402\n",
    "\n",
    "import galsim  # noqa: E402\n",
    "import jax.numpy as jnp  # noqa: E402\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import numpy as np  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf2a2e",
   "metadata": {},
   "source": [
    "# Fit a Psuedo-Pade Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940187b9",
   "metadata": {},
   "source": [
    "## Define the Approximation and Fitting Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0d47ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of rational function in log(maxk_threshold), log(beta)\n",
    "PADE_ORDERS = [9, 11]\n",
    "\n",
    "N_PARAMS_MKTS = PADE_ORDERS[0] * 2 - 1\n",
    "N_PARAMS_BETA = PADE_ORDERS[1] * 2 - 1\n",
    "N_PARAMS = N_PARAMS_MKTS * N_PARAMS_BETA\n",
    "\n",
    "LOG_BETA_MIN = np.log(1.1 + 1e-6)\n",
    "LOG_BETA_MAX = np.log(100)\n",
    "LOG_MKTS_MIN = np.log(1e-12)\n",
    "LOG_MKTS_MAX = np.log(0.1)\n",
    "\n",
    "\n",
    "def _pade_func(coeffs, x):\n",
    "    order = (coeffs.shape[0] - 1) // 2\n",
    "    p = jnp.polyval(coeffs[:order], x)\n",
    "    q = jnp.polyval(\n",
    "        jnp.concatenate([coeffs[order:], jnp.ones(1)], axis=0),\n",
    "        x,\n",
    "    )\n",
    "    return p / q\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@partial(jax.vmap, in_axes=(0, 0, None))\n",
    "def _logmaxk_psuedo_pade_approx(log_beta, log_mkts, coeffs):\n",
    "    log_beta = (log_beta - LOG_BETA_MIN) / (LOG_BETA_MAX - LOG_BETA_MIN)\n",
    "    log_mkts = (log_mkts - LOG_MKTS_MIN) / (LOG_MKTS_MAX - LOG_MKTS_MIN)\n",
    "    coeffs = coeffs.reshape(N_PARAMS_MKTS, N_PARAMS_BETA)\n",
    "    pqvals = jax.vmap(_pade_func, in_axes=(0, None))(coeffs, log_beta)\n",
    "    return _pade_func(pqvals, log_mkts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e05ca8",
   "metadata": {},
   "source": [
    "## Do the Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664de5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_beta = 50\n",
    "n_mkts = 50\n",
    "\n",
    "\n",
    "# this is the function we are interpolating\n",
    "def _fun(beta, mkt):\n",
    "    return galsim.Moffat(\n",
    "        beta,\n",
    "        scale_radius=1.0\n",
    "    ).withGSParams(maxk_threshold=mkt).maxk\n",
    "\n",
    "\n",
    "_betas = np.logspace(np.log10(np.exp(LOG_BETA_MIN)), np.log10(np.exp(LOG_BETA_MAX)), n_beta)\n",
    "_mkts = np.logspace(np.log10(np.exp(LOG_MKTS_MIN)), np.log10(np.exp(LOG_MKTS_MAX)), n_mkts)\n",
    "\n",
    "betas = []\n",
    "mkts = []\n",
    "maxks = []\n",
    "for beta in _betas:\n",
    "    for mkt in _mkts:\n",
    "        betas.append(beta)\n",
    "        mkts.append(mkt)\n",
    "        maxks.append(\n",
    "            _fun(beta, mkt)\n",
    "        )\n",
    "betas = jnp.array(betas)\n",
    "mkts = jnp.array(mkts)\n",
    "maxks = jnp.array(maxks)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def _loss(coeffs, lnbetas, lnmaxk_thresholds, lnmaxks):\n",
    "    pvals = _logmaxk_psuedo_pade_approx(lnbetas, lnmaxk_thresholds, coeffs)\n",
    "    return jnp.mean((pvals - lnmaxks)**2)\n",
    "\n",
    "\n",
    "_vag_loss = jax.jit(jax.value_and_grad(_loss))\n",
    "_g_loss = jax.jit(jax.grad(_loss))\n",
    "_h_loss = jax.jit(jax.hessian(_loss))\n",
    "\n",
    "# generate an initial guess\n",
    "coeffs = jnp.ones(N_PARAMS) * 1e-6\n",
    "\n",
    "# args for loss\n",
    "lnb = jnp.log(betas)\n",
    "lnmkts = jnp.log(mkts)\n",
    "lnmaxks = jnp.log(maxks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a57bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.scipy.optimize as jspop\n",
    "import optax\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def _min_optax(\n",
    "    fun,\n",
    "    x0,\n",
    "    args=None,\n",
    "    maxiter=100_000,\n",
    "    learning_rate=1e-1,\n",
    "    method=\"adan\",\n",
    "    optimizer=None,\n",
    "    opt_state=None,\n",
    "    update_prog_iter=100,\n",
    "):\n",
    "    args = args or tuple()\n",
    "    _vag_fun = jax.jit(jax.value_and_grad(fun))\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = getattr(optax, method)(learning_rate)\n",
    "        opt_state = optimizer.init(x0)\n",
    "\n",
    "        @jax.jit\n",
    "        def _update_func(coeffs, opt_state):\n",
    "            loss, grads = _vag_fun(coeffs, *args)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state, params=coeffs)\n",
    "            coeffs = optax.apply_updates(coeffs, updates)\n",
    "            return coeffs, opt_state, loss\n",
    "\n",
    "    loss, _ = _vag_fun(x0, *args)\n",
    "\n",
    "    prev_loss = None\n",
    "    coeffs = x0\n",
    "\n",
    "    with tqdm.trange(maxiter) as pbar:\n",
    "        for i in pbar:\n",
    "            coeffs, opt_state, loss = _update_func(coeffs, opt_state)\n",
    "\n",
    "            if i % update_prog_iter == 0 or i == 0:\n",
    "                if prev_loss is not None:\n",
    "                    dloss = loss - prev_loss\n",
    "                else:\n",
    "                    dloss = np.nan\n",
    "\n",
    "                pbar.set_description(f\"{method}: {loss:12.8e} ({dloss:+9.2e} delta)\")\n",
    "\n",
    "                prev_loss = loss\n",
    "\n",
    "    return coeffs, (optimizer, opt_state)\n",
    "\n",
    "\n",
    "def _min_bfgs(\n",
    "    fun,\n",
    "    x0,\n",
    "    args=None,\n",
    "    maxiter=100,\n",
    "):\n",
    "    args = args or tuple()\n",
    "\n",
    "    coeffs = x0\n",
    "    prev_loss = None\n",
    "    tol = 1e-16\n",
    "    with tqdm.trange(maxiter) as pbar:\n",
    "        for _ in pbar:\n",
    "            res = jspop.minimize(\n",
    "                fun,\n",
    "                coeffs,\n",
    "                method=\"BFGS\",\n",
    "                args=args,\n",
    "                tol=tol,\n",
    "                options={\"maxiter\": 10000, \"gtol\": tol, \"line_search_maxiter\": 40},\n",
    "            )\n",
    "\n",
    "            if np.all(coeffs == res.x):\n",
    "                coeffs = coeffs * (1.0 + (np.random.uniform(size=coeffs.shape[0]) - 0.5) * 1e-10)\n",
    "            else:\n",
    "                coeffs = res.x\n",
    "\n",
    "            if prev_loss is not None:\n",
    "                dloss = res.fun - prev_loss\n",
    "            else:\n",
    "                dloss = np.nan\n",
    "\n",
    "            prev_loss = res.fun\n",
    "\n",
    "            pbar.set_description(\n",
    "                f\"bfgs: {res.fun:12.8e} ({dloss:+9.2e} delta, status {res.status}, nit {res.nit:6d})\"\n",
    "            )\n",
    "\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b417d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 1.14907850e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bfgs: 3.34256538e-09 (-1.36e-10 delta, status 3, nit   1056): 100%|██████████| 100/100 [03:20<00:00,  2.01s/it]\n",
      "bfgs: 5.03995873e-10 (-2.06e-12 delta, status 3, nit    884): 100%|██████████| 100/100 [04:04<00:00,  2.45s/it]\n",
      "bfgs: 2.26712315e-10 (-1.09e-12 delta, status 3, nit    972):  94%|█████████▍| 94/100 [04:31<00:16,  2.82s/it]"
     ]
    }
   ],
   "source": [
    "args = (lnb, lnmkts, lnmaxks)\n",
    "\n",
    "loss = _loss(coeffs, *args)\n",
    "print(f\"initial loss: {loss:12.8e}\", flush=True)\n",
    "\n",
    "for _ in range(10):\n",
    "    # coeffs, _ = _min_optax(\n",
    "    #     _loss,\n",
    "    #     coeffs,\n",
    "    #     args=args,\n",
    "    #     learning_rate=1e-4,\n",
    "    #     maxiter=100_000,\n",
    "    #     update_prog_iter=1000,\n",
    "    # )\n",
    "    coeffs = _min_bfgs(\n",
    "        _loss,\n",
    "        coeffs,\n",
    "        args=args,\n",
    "        maxiter=100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab658c",
   "metadata": {},
   "source": [
    "## Print the Coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "pstr = textwrap.indent(\n",
    "    np.array2string(np.array(coeffs), floatmode=\"unique\", threshold=100000000, separator=\", \", max_line_width=120, sign=\"+\"),\n",
    "    \"    \",\n",
    ")\n",
    "\n",
    "code_str = \"\"\"\\\n",
    "# RATIONAL_POLY_VALS is the array of rational function\n",
    "# polynomial coefficients that define the approximation\n",
    "# fmt: off\n",
    "RATIONAL_POLY_VALS = np.array(\n",
    "{pstr},\n",
    "    dtype=np.float64,\n",
    ")\n",
    "# fmt: on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108951d9",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "n_test = 100000\n",
    "\n",
    "tbetas = 10**rng.uniform(low=np.log10(1.1 + 1e-6), high=np.log10(100), size=n_test)\n",
    "tmaxk_thresholds = 10**rng.uniform(low=-12, high=-1, size=n_test)\n",
    "apprx = jnp.exp(_logmaxk_psuedo_pade_approx(jnp.log(tbetas), jnp.log(tmaxk_thresholds), coeffs))\n",
    "true = np.array([\n",
    "    _fun(tbetas[i], tmaxk_thresholds[i])\n",
    "    for i in range(n_test)\n",
    "])\n",
    "eps = np.abs(apprx / true - 1)\n",
    "c_func = np.max\n",
    "eps_label = \"max|approx/true - 1|\"\n",
    "print(c_func(eps))\n",
    "\n",
    "msk = tmaxk_thresholds <= 0.01\n",
    "print(c_func(eps[msk]))\n",
    "\n",
    "# plt.hist(true / apprx - 1, bins=25, log=True)\n",
    "# ax = plt.gca()\n",
    "# ax.set_xlabel(\"fractional error in maxk approx.\")\n",
    "# ax.set_ylabel(\"# of points\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hb = ax.hexbin(\n",
    "    np.log10(tbetas),\n",
    "    np.log10(tmaxk_thresholds),\n",
    "    C=eps,\n",
    "    reduce_C_function=c_func,\n",
    "    extent=(np.log10(1.1), np.log10(100), -12, -1),\n",
    "    gridsize=50,\n",
    "    bins=\"log\",\n",
    ")\n",
    "ax.set_xlim(np.log10(1.1), np.log10(100))\n",
    "ax.set_ylim(-12, -1)\n",
    "ax.set_xlabel(\"log10(beta)\")\n",
    "ax.set_ylabel(\"log10(maxk_threshold)\")\n",
    "fig.colorbar(hb, label=eps_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1bd2c",
   "metadata": {},
   "source": [
    "# Hacking and Testing Code Below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dff6ba",
   "metadata": {},
   "source": [
    "## Define Range of Interpolant and Spacings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b33ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_min = 1.1 + 1e-6\n",
    "beta_max = 100\n",
    "n_beta = 100  # used to fit the rational function approx\n",
    "mkts_min = 1e-12\n",
    "mkts_max = 0.1\n",
    "n_mkts = 100  # we build this many rational function approximations\n",
    "RATNL_ORDER = 11\n",
    "\n",
    "betas = np.logspace(np.log10(beta_min), np.log10(beta_max), n_beta)\n",
    "mkts = np.logspace(jnp.log10(mkts_min), jnp.log10(mkts_max), n_mkts)\n",
    "\n",
    "# this is the function we are interpolating\n",
    "def _fun(beta, sr, mkt):\n",
    "    return galsim.Moffat(\n",
    "        beta,\n",
    "        scale_radius=sr\n",
    "    ).withGSParams(maxk_threshold=mkt).maxk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bf87a",
   "metadata": {},
   "source": [
    "## Build rational function apprx. in beta at fixed maxk_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "import numpy.polynomial\n",
    "from numpy.polynomial import Polynomial\n",
    "import jax_galsim.core.interpolate\n",
    "\n",
    "numpy.polynomial.set_default_printstyle(\"ascii\")\n",
    "\n",
    "\n",
    "def get_ratnl_func_polys(coeff):\n",
    "    p_coeff = coeff[0 : RATNL_ORDER + 1]\n",
    "    q_coeff = np.concatenate([[1], coeff[RATNL_ORDER + 1 :]])\n",
    "    pm = Polynomial(p_coeff)\n",
    "    qm = Polynomial(q_coeff)\n",
    "    return pm, qm\n",
    "\n",
    "\n",
    "def get_ratnl_func_coeffs(coeff):\n",
    "    p_coeff = coeff[0 : RATNL_ORDER + 1]\n",
    "    q_coeff = np.concatenate([[1], coeff[RATNL_ORDER + 1 :]])\n",
    "    return p_coeff, q_coeff\n",
    "\n",
    "\n",
    "def ratnl_func(x, *coeff):\n",
    "    pm, qm = get_ratnl_func_polys(coeff)\n",
    "    return pm(x) / qm(x)\n",
    "\n",
    "\n",
    "def make_poly_code(pm, head=\"\", base_indent=0):\n",
    "    res = \"\"\n",
    "    indent = base_indent\n",
    "    for c in pm.coef:\n",
    "        if c == pm.coef[-1]:\n",
    "            end = \"\"\n",
    "        else:\n",
    "            end = \" + x * (\"\n",
    "\n",
    "        if c == pm.coef[0]:\n",
    "            _hd = head\n",
    "        else:\n",
    "            _hd = \"\"\n",
    "        res += \" \" * 4 * indent + f\"{_hd}{c}{end}\\n\"\n",
    "        if c != pm.coef[-1]:\n",
    "            indent += 1\n",
    "\n",
    "    for _ in pm.coef[:-1]:\n",
    "        indent -= 1\n",
    "        res += \" \" * 4 * indent + \")\\n\"\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be97104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "\n",
    "poly_res = []\n",
    "\n",
    "for mkt in mkts:\n",
    "    vals = np.array([_fun(beta, 1.0, mkt) for beta in betas])\n",
    "\n",
    "    res = scipy.optimize.curve_fit(\n",
    "        ratnl_func,\n",
    "        np.log(betas),\n",
    "        np.log(vals),\n",
    "        p0=np.ones(2 * RATNL_ORDER + 1),\n",
    "        full_output=True,\n",
    "        maxfev=100000,\n",
    "        ftol=eps,\n",
    "        xtol=eps,\n",
    "    )\n",
    "\n",
    "    coeff = res[0]\n",
    "\n",
    "    pm, qm = get_ratnl_func_coeffs(coeff)\n",
    "\n",
    "    poly_res.append((pm[::-1], qm[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "pstr = textwrap.indent(\n",
    "    np.array2string(np.array(poly_res), floatmode=\"unique\", threshold=100000000, separator=\", \", max_line_width=120, sign=\"+\"),\n",
    "    \"    \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = f\"\"\"\\\n",
    "# START OF GENERATED CODE\n",
    "# The code in this block is generated by the notebook dev/notebooks/moffat_maxk_interp.ipynb.\n",
    "\n",
    "MKTS_MIN = {mkts_min}\n",
    "MKTS_MAX = {mkts_max}\n",
    "N_MKTS = {n_mkts}\n",
    "LOG_MKTS = np.log(np.logspace(jnp.log10(MKTS_MIN), jnp.log10(MKTS_MAX), N_MKTS))\n",
    "\n",
    "# RATIONAL_POLY_VALS is an array of 7-th order ration function approximations\n",
    "# for maxk as a function of log(beta) at fixed maxk_threshold values. the coeffs\n",
    "# are stored from highest degree to lowest. The shape of the array is\n",
    "# ({n_mkts}, 2, {RATNL_ORDER + 1}).\n",
    "# fmt: off\n",
    "RATIONAL_POLY_VALS = np.array(\n",
    "{pstr},\n",
    "    dtype=np.float64,\n",
    ")\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def _moffat_maxk(beta, maxk_threshold, r0):\n",
    "    log_beta = jnp.log(beta)\n",
    "    log_maxk_threshold = jnp.log(maxk_threshold)\n",
    "    maxk_vals = jnp.array(\n",
    "        [\n",
    "            jnp.exp(\n",
    "                jnp.polyval(RATIONAL_POLY_VALS[i, 0, :], log_beta)\n",
    "                / jnp.polyval(RATIONAL_POLY_VALS[i, 1, :], log_beta)\n",
    "            )\n",
    "            for i in range(N_MKTS)\n",
    "        ]\n",
    "    )\n",
    "    coeffs = akima_interp_coeffs(LOG_MKTS, maxk_vals)\n",
    "    return akima_interp(log_maxk_threshold, LOG_MKTS, maxk_vals, coeffs) / r0\n",
    "\n",
    "\n",
    "# END OF GENERATED CODE\n",
    "\"\"\"\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252b268",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax_galsim.core.interpolate import akima_interp, akima_interp_coeffs\n",
    "\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=10)\n",
    "n_test = 100000\n",
    "\n",
    "betas = rng.uniform(low=1.1 + 1e-6, high=100, size=n_test)\n",
    "maxk_thresholds = 10**rng.uniform(low=-12, high=-1, size=n_test)\n",
    "apprx = np.array([\n",
    "    _moffat_maxk(betas[i], maxk_thresholds[i], 1.0)\n",
    "    for i in range(n_test)\n",
    "])\n",
    "true = np.array([\n",
    "    _fun(betas[i], 1.0, maxk_thresholds[i])\n",
    "    for i in range(n_test)\n",
    "])\n",
    "\n",
    "# plt.hist(true / apprx - 1, bins=25, log=True)\n",
    "# ax = plt.gca()\n",
    "# ax.set_xlabel(\"fractional error in maxk approx.\")\n",
    "# ax.set_ylabel(\"# of points\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hb = ax.hexbin(\n",
    "    betas,\n",
    "    np.log10(maxk_thresholds),\n",
    "    C=np.log10(np.abs(apprx-true)),\n",
    "    extent=(1.1, 100, -12, -1),\n",
    "    gridsize=50,\n",
    "    vmin=-7,\n",
    "    vmax=-2\n",
    ")\n",
    "ax.set_xlim(1.1, 100)\n",
    "ax.set_ylim(-12, -1)\n",
    "ax.set_xlabel(\"beta\")\n",
    "ax.set_ylabel(\"log10(maxk_threshold)\")\n",
    "fig.colorbar(hb, label=\"log10(|approx - true|)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396f567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82279445",
   "metadata": {},
   "source": [
    "## Code to Minimize w/ SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import tqdm\n",
    "\n",
    "optimizer = None\n",
    "learning_rate = 1e-5\n",
    "opt_attr = \"adan\"\n",
    "\n",
    "if optimizer is None:\n",
    "    optimizer = getattr(optax, opt_attr)(learning_rate)\n",
    "    opt_state = optimizer.init(coeffs)\n",
    "\n",
    "    @jax.jit\n",
    "    def _update_func(coeffs, opt_state):\n",
    "        loss, grads = _vag_loss(coeffs, lnb, lnmkts, lnmaxks)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, params=coeffs)\n",
    "        coeffs = optax.apply_updates(coeffs, updates)\n",
    "        return coeffs, opt_state, loss\n",
    "\n",
    "lnb = jnp.log(betas)\n",
    "lnmkts = jnp.log(mkts)\n",
    "lnmaxks = jnp.log(maxks)\n",
    "\n",
    "loss, _ = _vag_loss(coeffs, lnb, lnmkts, lnmaxks)\n",
    "print(\"initial loss:\", jnp.power(loss, 1.0 / lval), flush=True)\n",
    "\n",
    "prev_loss = None\n",
    "n_epoch = 200_000\n",
    "ditr = 1000\n",
    "\n",
    "with tqdm.trange(n_epoch) as pbar:\n",
    "    for i in pbar:\n",
    "        coeffs, opt_state, loss = _update_func(coeffs, opt_state)\n",
    "\n",
    "        if i % ditr == 0:\n",
    "            if prev_loss is not None:\n",
    "                dloss = (jnp.power(loss, 1 / lval) - jnp.power(prev_loss, 1 / lval))\n",
    "                pbar.set_description(f\"loss: {jnp.power(loss, 1 / lval):10.4e} ({dloss:+9.2e} delta)\")\n",
    "            else:\n",
    "                pbar.set_description(f\"loss: {jnp.power(loss, 1 / lval):10.4e} (--------- delta)\")\n",
    "\n",
    "            prev_loss = loss\n",
    "\n",
    "print(f\"{i:04d}: {jnp.power(loss, 1 / lval):10.4e}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f152994",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function we are interpolating\n",
    "def _fun(beta, mkt):\n",
    "    return galsim.Moffat(\n",
    "        beta,\n",
    "        scale_radius=1.0\n",
    "    ).withGSParams(maxk_threshold=mkt).maxk\n",
    "\n",
    "\n",
    "beta_min = 1.1 + 1e-6\n",
    "beta_max = 100\n",
    "n_beta = 500  # used to fit the rational function approx\n",
    "mkts_min = 1e-12\n",
    "mkts_max = 0.1\n",
    "n_mkts = 200  # we build this many rational function approximations\n",
    "\n",
    "_betas = np.logspace(np.log10(beta_min), np.log10(beta_max), n_beta)\n",
    "_mkts = np.logspace(jnp.log10(mkts_min), jnp.log10(mkts_max), n_mkts)\n",
    "\n",
    "betas = []\n",
    "mkts = []\n",
    "maxks = []\n",
    "for beta in _betas:\n",
    "    for mkt in _mkts:\n",
    "        betas.append(beta)\n",
    "        mkts.append(mkt)\n",
    "        maxks.append(\n",
    "            _fun(beta, mkt)\n",
    "        )\n",
    "betas = jnp.array(betas)\n",
    "mkts = jnp.array(mkts)\n",
    "maxks = jnp.array(maxks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c5ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import interpax\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "n_test = 100000\n",
    "\n",
    "tbetas = 10**rng.uniform(low=np.log10(1.1 + 1e-6), high=np.log10(100), size=n_test)\n",
    "tmaxk_thresholds = 10**rng.uniform(low=-12, high=-1, size=n_test)\n",
    "apprx = jnp.exp(\n",
    "    interpax.interp2d(\n",
    "        jnp.log(tbetas),\n",
    "        jnp.log(tmaxk_thresholds),\n",
    "        jnp.log(_betas),\n",
    "        jnp.log(_mkts),\n",
    "        jnp.log(maxks).reshape(n_beta, n_mkts),\n",
    "        method=\"akima\"\n",
    "    )\n",
    ")\n",
    "true = np.array([\n",
    "    _fun(tbetas[i], tmaxk_thresholds[i])\n",
    "    for i in range(n_test)\n",
    "])\n",
    "eps = np.abs(apprx / true - 1)\n",
    "c_func = np.max\n",
    "eps_label = \"max|approx/true - 1|\"\n",
    "print(c_func(eps))\n",
    "\n",
    "msk = tmaxk_thresholds <= 0.03\n",
    "print(c_func(eps[msk]))\n",
    "# plt.hist(true / apprx - 1, bins=25, log=True)\n",
    "# ax = plt.gca()\n",
    "# ax.set_xlabel(\"fractional error in maxk approx.\")\n",
    "# ax.set_ylabel(\"# of points\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hb = ax.hexbin(\n",
    "    np.log10(tbetas),\n",
    "    np.log10(tmaxk_thresholds),\n",
    "    C=eps,\n",
    "    reduce_C_function=c_func,\n",
    "    extent=(np.log10(1.1), np.log10(100), -12, -1),\n",
    "    gridsize=50,\n",
    "    bins=\"log\",\n",
    ")\n",
    "ax.set_xlim(np.log10(1.1), np.log10(100))\n",
    "ax.set_ylim(-12, -1)\n",
    "ax.set_xlabel(\"log10(beta)\")\n",
    "ax.set_ylabel(\"log10(maxk_threshold)\")\n",
    "fig.colorbar(hb, label=eps_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a5c3e",
   "metadata": {},
   "source": [
    "## Symbolic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fec48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function we are interpolating\n",
    "def _fun(beta, mkt):\n",
    "    return galsim.Moffat(\n",
    "        beta,\n",
    "        scale_radius=1.0\n",
    "    ).withGSParams(maxk_threshold=mkt).maxk\n",
    "\n",
    "\n",
    "beta_min = 1.1 + 1e-6\n",
    "beta_max = 100\n",
    "n_beta = 50  # used to fit the rational function approx\n",
    "mkts_min = 1e-12\n",
    "mkts_max = 0.1\n",
    "n_mkts = 50  # we build this many rational function approximations\n",
    "\n",
    "_betas = np.logspace(np.log10(beta_min), np.log10(beta_max), n_beta)\n",
    "_mkts = np.logspace(jnp.log10(mkts_min), jnp.log10(mkts_max), n_mkts)\n",
    "\n",
    "betas = []\n",
    "mkts = []\n",
    "maxks = []\n",
    "for beta in _betas:\n",
    "    for mkt in _mkts:\n",
    "        betas.append(beta)\n",
    "        mkts.append(mkt)\n",
    "        maxks.append(\n",
    "            _fun(beta, mkt)\n",
    "        )\n",
    "betas = jnp.array(betas)\n",
    "mkts = jnp.array(mkts)\n",
    "maxks = jnp.array(maxks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5298f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysr import PySRRegressor\n",
    "\n",
    "X = np.stack([np.log(betas), np.log(mkts)], axis=1)\n",
    "y = np.log(maxks)\n",
    "\n",
    "model = PySRRegressor(\n",
    "    maxsize=50,\n",
    "    niterations=100,\n",
    "    binary_operators=[\"+\", \"*\", \"/\", \"^\"],\n",
    "    constraints={'^': (-1, 1)},\n",
    "    elementwise_loss=\"loss(prediction, target) = abs(prediction - target)\",\n",
    "    model_selection='accuracy',\n",
    ")\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbecb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "n_test = 100000\n",
    "\n",
    "tbetas = 10**rng.uniform(low=np.log10(1.1 + 1e-6), high=np.log10(100), size=n_test)\n",
    "tmaxk_thresholds = 10**rng.uniform(low=-12, high=-1, size=n_test)\n",
    "tX = np.stack([np.log(tbetas), np.log(tmaxk_thresholds)], axis=1)\n",
    "\n",
    "apprx = np.exp(model.predict(tX))\n",
    "true = np.array([\n",
    "    _fun(tbetas[i], tmaxk_thresholds[i])\n",
    "    for i in range(n_test)\n",
    "])\n",
    "eps = np.abs(apprx / true - 1)\n",
    "c_func = np.max\n",
    "eps_label = \"max|approx/true - 1|\"\n",
    "print(c_func(eps))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "hb = ax.hexbin(\n",
    "    np.log10(tbetas),\n",
    "    np.log10(tmaxk_thresholds),\n",
    "    C=eps,\n",
    "    reduce_C_function=c_func,\n",
    "    extent=(np.log10(1.1), np.log10(100), -12, -1),\n",
    "    gridsize=50,\n",
    "    bins=\"log\",\n",
    ")\n",
    "ax.set_xlim(np.log10(1.1), np.log10(100))\n",
    "ax.set_ylim(-12, -1)\n",
    "ax.set_xlabel(\"log10(beta)\")\n",
    "ax.set_ylabel(\"log10(maxk_threshold)\")\n",
    "fig.colorbar(hb, label=eps_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549f6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.log(_betas * 1.01)\n",
    "y = jnp.ones_like(x) * jnp.log(1e-4)\n",
    "true = jnp.array([\n",
    "    _fun(jnp.exp(x[i]), jnp.exp(y[i]))\n",
    "    for i in range(x.shape[0])\n",
    "])\n",
    "\n",
    "approx = (\n",
    "    interpax.interp2d(\n",
    "        x,\n",
    "        y,\n",
    "        jnp.log(_betas),\n",
    "        jnp.log(_mkts),\n",
    "        jnp.log(maxks).reshape(n_beta, n_mkts),\n",
    "        method=\"akima\"\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    x,\n",
    "    approx,\n",
    ")\n",
    "plt.plot(\n",
    "    x,\n",
    "    jnp.log(true),\n",
    ")\n",
    "\n",
    "# plt.plot(x, jnp.log(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpax.interp2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2be7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-galsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
